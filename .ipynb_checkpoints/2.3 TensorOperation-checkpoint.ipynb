{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "[[0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]]\n",
      "[[0. 1. 2.]\n",
      " [3. 4. 5.]]\n",
      "(20, 300)\n"
     ]
    }
   ],
   "source": [
    "#In the first example we did on 2.1 chapter, we created a neural network by stacking dense layers\n",
    "#keras.layers.Dense(512, activation='relu')\n",
    "\n",
    "\"\"\"\n",
    "This layer can be interpretted as a function that takes 2D tensor as an input and returns another 2D tensor, which is a new \n",
    "representation of input tensor. Specifically, this function is like this: output = relu(dot(W, input) + b) where W is 2D tensor, and b\n",
    "is a vector.\n",
    "\n",
    "Below are three tensor operations: dot operation between input tensor and tensor W, addition between 2D tensor which is a result of dot\n",
    "operation and vector b, and relu. relu(x) is equivalent to max(x, 0)\n",
    "\n",
    "Relu function and addition is element-wise operation. Element-wise operation is applied on each elements in tensor independently.\n",
    "\"\"\"\n",
    "\n",
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x\n",
    "\n",
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#addition\n",
    "#z = x + y\n",
    "\n",
    "#relu function\n",
    "#z = np.maximum(z, 0.)\n",
    "\n",
    "\"\"\"\n",
    "If you try to add two tensors with different size, broadcasting occurs. Broadcasting consists of two steps:\n",
    "1. Axis are added to \"small\" tensor so that small tensor's ndim matches that of big tensor\n",
    "2. Small tensor is then replicated across the new axis to match the size of big tensor\n",
    "\n",
    "For example, suppose that we are trying to do:\n",
    "[[1, 2, 3],  +  [7, 8, 9]\n",
    " [4, 5, 6]]\n",
    "\n",
    "To compute this, we need to broadcast the latter tensor like this:\n",
    "[[1, 2, 3],  +  [[7, 8, 9],\n",
    " [4, 5, 6]]      [7, 8, 9]]\n",
    "\n",
    "In implementing this, if actually new tensor is created in the process, it would be very inefficient. Therefore, this new 2D tensor\n",
    "after broadcasting is not actually created in memory.\n",
    "\"\"\"\n",
    "\n",
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "    return x\n",
    "\n",
    "#You can apply broadcasting to (a,b,...,m,...,n) size tensor and (m,...,n) size tensor. Broadcasting occurs at a to n-1 axis.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.random((64, 3, 32, 10))\n",
    "y = np.random.random((32, 10))\n",
    "\n",
    "z = np.maximum(x, y)\n",
    "\n",
    "\"\"\"\n",
    "Dot operation, aka tensor product, is widely used tensor operation.\n",
    "\"\"\"\n",
    "\n",
    "#import numpy as np\n",
    "#z = np.dot(x,y)\n",
    "\n",
    "#Dot operation between two vectors\n",
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    \n",
    "    z = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z\n",
    "\n",
    "#Dot operation between one matrix and one vector\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros(x.shape[0]) #This creates vector with same size as x, with elements all zeros\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z\n",
    "\n",
    "\"\"\"\n",
    "We can also write this as:\n",
    "\n",
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z\n",
    "\"\"\"\n",
    "\n",
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z\n",
    "\n",
    "\"\"\"\n",
    "Tensor reshaping is restructing the row and column of the tensor. Reshaped tensor has same number of elements with the original's.\n",
    "\"\"\"\n",
    "\n",
    "#Example of tensor reshaping\n",
    "x = np.array([[0., 1.],\n",
    "              [2., 3.],\n",
    "              [4., 5.]])\n",
    "print(x.shape)\n",
    "x = x.reshape((6, 1))\n",
    "print(x)\n",
    "x = x.reshape((2, 3))\n",
    "print(x)\n",
    "\n",
    "#Frequently used tensor reshaping is transposition. It changes row and column. In other words, x[i, :] becomes x[:, i] and vice versa.\n",
    "x = np.zeros((300, 20))\n",
    "x = np.transpose(x)\n",
    "print(x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
