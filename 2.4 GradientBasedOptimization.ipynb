{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As we have seen before, each layer in the neural network example in 2.1 converts input data according to this equation:\n",
    "output = relu(dot(W, input) + b)\n",
    "In this equation, tensor W and b can be viewed as a layer's attribute. This is called weight, or trainable parameter (also called \n",
    "kernel or bias). Weight contains information from training neural network with training data.\n",
    "\n",
    "Weight tensors are randomly initialized at first (random initialization). Of course, we would not expect relu(dot(W, input) + b) would\n",
    "output meaningful expression when W and b are random. However, as training continues and it gets feedback signal, weight will gradually\n",
    "be adjusted. This gradual adjusting, or training, is the key to the machine learning.\n",
    "\n",
    "Training happens inside a training loop. These steps are repeated as needed:\n",
    "1. Extract the batch of training sample x and corresponding target y.\n",
    "2. Find prediction y_pred by running the network (forward pass) using x.\n",
    "3. Calculate the loss value for this batch by measuring the difference between y and y_pred.\n",
    "4. Update all of the network's weight so that the loss value for the particular batch will be decreased.\n",
    "\n",
    "Step 4 is the hard part. One way to do this is to make all other weights constant and try changing one weight that we are interested\n",
    "with. However, this is inefficient since normally we have thousands and even millions of weights. Therefore, we will use the fact that\n",
    "all operations used in neural network is differentiable, and calculate gradient of loss value over network weights instead.\n",
    "\n",
    "If a differentiable function is given, we can theoretically find its minimum analytically. If we apply this into neural network, we can\n",
    "find the combination of values of weights that makes the smallest loss function by solving gradient(f)(W), where f is a function that \n",
    "maps W into loss value. This is a polynomial that is consisted with N variables, where N is number of weights in the network. However,\n",
    "since there can be thousands of weights in a network, it is very hard to solve this equation analytically.\n",
    "\n",
    "We can instead use 4-step algorithm shown above:\n",
    "1. Extract the batch of training sample x and corresponding target y.\n",
    "2. Find prediction y_pred by running the network using x.\n",
    "3. Calculate the loss value for this batch by measuring the difference between y and y_pred.\n",
    "4. Calculate the gradient of loss function for the network's parameters (backward pass)\n",
    "5. Move parameters a little bit in the opposite direction of the gradient (ex. W -= step * gradient)\n",
    "\n",
    "This procedure is called mini-batch stochastic gradient descent (mini-batch SGD). Stochastic means that each batch data is randomly\n",
    "selected. Another variety of SGD algorithm is true SGD, which uses one sample and one target per repetition instead of batch. Another\n",
    "one is batch SGD, which uses all of the data per repetition.\n",
    "\n",
    "Furthermore, there are more varieties of SGD that not only consider current gradient value, but also other weights that has been \n",
    "updated. For example, there are SGD that uses momentum, Adagard, and RMSProp. These varieties are all called optimization method or \n",
    "optimizer. Momentum helps solve two problems SGD has: conversion speed and local minimum.\n",
    "\n",
    "If there exists a local minimum in loss function, SGD process can be trapped in the local minimum, not being able to reach global \n",
    "minimum. We can understand momentum as rolling a small ball on the loss function curve. If there is enough momentum, the ball would not \n",
    "be trapped in the local minimum, and reach global minimum. Momentum not only considers current acceleration, but also current speed to \n",
    "move the ball. In optimization method, it not only considers current gradient value but also parameters that were updated before to \n",
    "update parameter w. For example:\n",
    "\n",
    "past_velocity = 0.\n",
    "momentum = 0.1\n",
    "while loss > 0.01:\n",
    "    w, loss, gradient = get_current_parameters()\n",
    "    velocity = momentum * past_velocity - learning_rate * gradient\n",
    "    w = w + momentum * velocity - learning rate * gradient\n",
    "    past_velocity = velocity\n",
    "    update_paramenter(w)\n",
    "    \n",
    "Previously we assumed that we can calculate gradient because the operations in neural network are differentiable. Neural network \n",
    "consists of many tensor operations, and tensor operations' rate of change is simple and well known. Suppose that we have a network f\n",
    "that consists of three tensor operations a, b, and c, and weight tensor W1, W2, and W3:\n",
    "\n",
    "f(W1, W2, W3) = a(W1, b(W2, c(W3)))\n",
    "\n",
    "In calculus, we can use chain rule to find gradient(f) function. We have applied chain rule to gradient calculation in neural network to\n",
    "create backpropagation algorithm, aka reverse-mode automatic differentiation. Backpropagation starts at the final loss value, applying\n",
    "chain rule from topmost layer to bottom layers to calculate the amount each parameter contributed to the final loss value.\n",
    "\n",
    "Currently, we use framework that enables symbolic differentiation to implement neural network. This means that we use tensor operations\n",
    "whose rate of change is already known. If we use these operations, we can simplify the backward pass by calling the gradient function.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
